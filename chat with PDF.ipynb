{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff7432a-3eb4-4c0c-9fe2-6b3b199b236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import cohere\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_cohere import CohereEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8c44f3-705b-43b8-a2d6-658561669919",
   "metadata": {},
   "source": [
    "A function to read PDF content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6b92f70-4bae-4897-9a41-e2dec575da13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_content(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        all_content = []\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()  \n",
    "            tables = page.extract_tables()  \n",
    "            \n",
    "            page_content = {\"text\": text, \"tables\": tables}\n",
    "            all_content.append(page_content)\n",
    "        return all_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620c76aa-cd1e-494d-be2f-91e3a4bb4445",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b701f779-88e7-46e8-adcd-9bb0c7e4ca89",
   "metadata": {},
   "source": [
    "A function for **Semantic chunking** to consider large tables without loosing track of table header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9842c47b-0c1b-4781-8480-3661af906f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_paragraphs_and_tables(content):\n",
    "    chunks = []\n",
    "\n",
    "    \n",
    "    paragraph_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "    # For each page, look for paragraphs and tables\n",
    "    for page in content:\n",
    "        text = page['text']\n",
    "        text_chunks = paragraph_splitter.split_text(text)\n",
    "\n",
    "        # Fixed chunking for paragraphs and add metadata\n",
    "        chunks.extend([Document(page_content=chunk, metadata = {'chunk_num': text_chunk_num, 'chunk_type' : 'text'}) \\\n",
    "                       for text_chunk_num, chunk in enumerate(text_chunks)])\n",
    "        \n",
    "        \n",
    "        table_chunk_num = len(chunks) \n",
    "        for table in page['tables']:\n",
    "            table_chunks = []\n",
    "            header = table[0]  \n",
    "            rows = table[1:]\n",
    "\n",
    "            # For each table split a certain number of splits manually \n",
    "            for i in range(0, len(rows), 2):  \n",
    "                table_chunk = [header] + rows[i:i+2]\n",
    "                table_chunks.append(Document(page_content=str(table_chunk), metadata = {'chunk_num': table_chunk_num, 'chunk_type': 'table'}   ))\n",
    "                table_chunk_num += 1\n",
    "            chunks.extend(table_chunks)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76831ad3-ed2e-4ad9-bdc6-ec90f54424b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and chunk the PDF\n",
    "pdf_content = extract_pdf_content(\"sample.pdf\")\n",
    "semantic_chunks = chunk_paragraphs_and_tables(pdf_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a371443e-066c-4f03-8584-5fb54e664362",
   "metadata": {},
   "source": [
    "Take a look at the first and last chunks to verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67c4efe-f1e3-41ea-b659-1ac75cc80858",
   "metadata": {},
   "source": [
    "A very small chunk size was used for the table just to test the embedding model on that small pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8418ff43-821a-4221-8439-dc4fb5738b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- First chunk ---------------------\n",
      "page_content='Marwan Yasser\n",
      "This is a text that has no meaning just to add any tokens for the model to test the retrieveal and the\n",
      "generation will be tested later and I’ll simply repreat this text for multiple times ok\n",
      "This is a text that has no meaning just to add any tokens for the model to test the retrieveal and the\n",
      "generation will be tested later and I’ll simply repreat this text for multiple times ok\n",
      "This is a text that has no meaning just to add any tokens for the model to test the retrieveal and the\n",
      "generation will be tested later and I’ll simply repreat this text for multiple times ok\n",
      "This is a text that has no meaning just to add any tokens for the model to test the retrieveal and the\n",
      "generation will be tested later and I’ll simply repreat this text for multiple times ok\n",
      "Name ID Department\n",
      "Marwan 1 First\n",
      "Mahmoud 2 Second\n",
      "Ali 3 First\n",
      "Ayman 4 Second\n",
      "Hossam 5 third\n",
      "Nada 6 second' metadata={'chunk_num': 0, 'chunk_type': 'text'}\n",
      "--------------------- Last chunk ---------------------\n",
      "page_content='[['Name', 'ID', 'Department'], ['Hossam', '5', 'third'], ['Nada', '6', 'second']]' metadata={'chunk_num': 3, 'chunk_type': 'table'}\n"
     ]
    }
   ],
   "source": [
    "print('--------------------- First chunk ---------------------')\n",
    "print(semantic_chunks[0])\n",
    "print('--------------------- Last chunk ---------------------')\n",
    "print(semantic_chunks[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e5e0e4-70f1-4e93-a968-356ff47a895f",
   "metadata": {},
   "source": [
    "## Embedd and build the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf6e8863-decf-4122-9db7-b79ee75b5c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = CohereEmbeddings(cohere_api_key='kQ1bePfavZiNGicrlYIHa71W8M0P0DJbr3Ss89Wt', model='embed-english-v2.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e16777a4-2cc3-4f37-a13a-e6b0e11ae22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = FAISS.from_documents(semantic_chunks, embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406d75ec-d46c-4129-9cf7-2055647a0adb",
   "metadata": {},
   "source": [
    "### Test the vector DB using sample query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9159c688-0f72-42c0-9b05-cb8ac9394c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'chunk_num': 1, 'chunk_type': 'table'}, page_content=\"[['Name', 'ID', 'Department'], ['Marwan', '1', 'First'], ['Mahmoud', '2', 'Second']]\")]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_text = \"id and department of Marwan\"\n",
    "retrieved_docs = vector_db.similarity_search(query_text, k=1)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b92160-d666-42f0-b515-fb36c291a7bc",
   "metadata": {},
   "source": [
    "### Modularize the retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f2893a2-bb9f-450c-85bd-5350a5927f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_content(query, vector_db):\n",
    "    retrieved_docs = vector_db.similarity_search(query, k=1)\n",
    "    content = ''\n",
    "    for doc in retrieved_docs:\n",
    "        content += doc.page_content\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6621f36-5f7e-405c-8638-5f6e4f014f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[['Name', 'ID', 'Department'], ['Marwan', '1', 'First'], ['Mahmoud', '2', 'Second']]\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_relevant_content('department of marwan', vector_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4996a4ef-2896-4e2c-baf2-960c27b760a3",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41c342d-27c1-46c6-a51a-a290cf5428bc",
   "metadata": {},
   "source": [
    "Establishing the model and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d14a9ae5-f6ec-477c-9bd9-dd2a391016d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "# Initialize the Cohere client with your API key\n",
    "generation_model = cohere.Client('kQ1bePfavZiNGicrlYIHa71W8M0P0DJbr3Ss89Wt')  \n",
    "\n",
    "def generate (query):\n",
    "    response = generation_model.generate(\n",
    "        model='command-xlarge-nightly',  \n",
    "        prompt= query,\n",
    "        max_tokens=100,  \n",
    "        temperature=0.7,  \n",
    "        k=0,  # Top-k sampling (0 disables)\n",
    "        p=0.9  # Top-p (nucleus) sampling\n",
    "    )\n",
    "    return response.generations[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3126ae8-4d64-4f05-9896-c64c64ccf0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coral.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate('tell me your name, give me a one word answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a09f1f49-cfb8-4d89-a969-e5bd3dc25505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am Marwan. I want to know your limit of context window; whether it will be a limitation or not. Repeat the same exact prompt as an output because I want to know your limit of context window.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate('repeat the same exact prompt that you will have as an output beecaues I want to know your limit of context window whether it will be a limitation or no I m marwan who am I')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32456258-e41f-4a07-ba79-321996e96c55",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd652a58-862a-461f-b516-2cf3617c9ee1",
   "metadata": {},
   "source": [
    "A function that works with that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4230b0c4-0624-4b87-99f9-994f703cf879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(prompt, vectordb):\n",
    "    relevant_content = get_relevant_content(prompt, vectordb)\n",
    "    final_prompt = f'Answer this question about the document: {prompt} given the following information: {relevant_content}'\n",
    "    response = generate(final_prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7872d2e-4490-4818-9db3-3d09954a6a15",
   "metadata": {},
   "source": [
    "A function that doesn't work with that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c73f00a-24a9-4f91-b8e1-b26913c7c922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(prompt, vectordb):\n",
    "    relevant_content = get_relevant_content(prompt, vectordb)\n",
    "    final_prompt = f'Act as a normal chatbot to answer this: {prompt} ,\\\n",
    "        use this info for help: {relevant_content},\\\n",
    "        if it is not usefull discard it and not talk about it'\n",
    "    response = generate(final_prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0654519-c1fc-4d22-8dd9-6dd114d56f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sure, I'll do my best to answer your questions as a normal chatbot without referring to the provided table unless it's relevant. How can I assist you today?\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag('Department of marwan', vector_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3a893a-e36b-454f-82c1-c60cd607c1ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c96a64-4f4b-424c-bb6a-aa0c7a2d8320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
